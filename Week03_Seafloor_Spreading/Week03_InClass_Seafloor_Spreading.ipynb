{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Seafloor Spreading\n",
    "\n",
    "**Last week we:**\n",
    "- Loaded and visualized an earthquake catalog.\n",
    "- Plotted earthquake magnitude and depth.\n",
    "- Learned some more complicated mapping techinque. \n",
    "\n",
    "**Our goals for today:**\n",
    "- pandas DataFrames, indexing, and data cleaning.\n",
    "- Load marine geophysical data (bathymetry and marine magnetic anomalies) from two oceanic ridges.\n",
    "- Select data and drop rows with gaps.\n",
    "- Plot bathymetry data and evaluate spreading rate.\n",
    "- Declare a function to detrend and filter magnetic anomaly data.\n",
    "- Plot marine magnetic anomaly data and compare spreading rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell as it is to setup your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from cartopy import config\n",
    "import cartopy.crs as ccrs\n",
    "from scipy import signal\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part 1: Data Wrangling\n",
    "\n",
    "### Arrays and DataFrames\n",
    "\n",
    "NumPy and pandas offer several types of data structures, the two main structures that we have been using so far and will use in future are `nparray` and `DataFrame`. A `nparray` is a fast and flexible container for large datasets that allows you to perform operations on whole blocks of data at once. Arrays are best suited for homogenous (just one type) numerical data. `DataFrames` are designed for tabular datasets, and can handle heterogenous data (multiple types: int, float, string, etc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__nparray__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random nparray called arr_data\n",
    "arr_data = np.random.randn(2,3)\n",
    "arr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use .shape to determine the shape of arr_data\n",
    "arr_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use .dtype to determine the type of arr_data\n",
    "arr_data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use .ndim to determine the dimensions of arr_data\n",
    "arr_data.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a nparray of zeros with np.zeros\n",
    "arr0 = np.zeros((4,4))\n",
    "arr0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a nparray of ones with np.ones\n",
    "arr1 = np.ones((4,4))\n",
    "arr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.ones is handy for making a nparray of any single value\n",
    "arr5 = arr1 * 5\n",
    "arr5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an array of integers between 0 and 10 in steps of 1, including 0 (start) but not 11 (end)\n",
    "arr2 = np.arange(0,11,1) \n",
    "arr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an array of floats between 0 and 10 in steps of 2, including 0 (start) but not 11 (end)\n",
    "arr2 = np.arange(0.0,11.0,2.0) \n",
    "arr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an array of 14 evenly spaced numbers between 0 and 10, including 0 (start) and 10 (end).\n",
    "arr3=np.linspace(0,10,14) \n",
    "arr3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DataFrame__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Series` and `DataFrames` are like nparrays but they have the added feature of index labels assigned to each row and column -- the bold labels in the below `DataFrame`. These labels can be used to bin and select data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a new DataFrame\n",
    "# note that index values (like the column labels) don't have to integers and don't have to be in order\n",
    "frame = pd.DataFrame(np.random.rand(3, 3), index=['Nevada','Montana','Arizona'], columns=['sedimentary','igneous','metamorphic'])\n",
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen `DataFrame` structures before in our tabular data files. The Earthquake catalog we were dealing with last week was a .csv (Comma Separated Variable) data file of all the earthquakes of magnitude 4 and higher from 2000 - 2012 in the ANSS (Advanced National Seismic System) Comprehensive Catalog or \"ComCat.\" We imported it as a DataFrame like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EQ_catalog = pd.read_csv('ANSS_2000_2012.csv',header=8,names = ['DateTime','Latitude','Longitude','Depth','Magnitude','MagType','NbStations','Gap','Distance','RMS','Source','EventID'])\n",
    "EQ_catalog.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have gained experience referencing individual columns (which are called `Series`): `DataFrame['Column_Name']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EQ_catalog['Depth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(EQ_catalog['Depth']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.values` function can be used to return the values of the `Series` as a `nparray`, so without the labled index values of the `Series`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EQ_catalog['Depth'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(EQ_catalog.Depth.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing and Slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/indices.png\" width=900>\n",
    "> Source: Python for Data Analysis (2nd Edition) McKinney, W."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/array_slicing.png\" width=900>\n",
    "> Source: Python for Data Analysis (2nd Edition) McKinney, W."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a random array\n",
    "arr_data = np.random.randn(10,5)\n",
    "arr_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=goldenrod>**_Code for you to write_**</font>\n",
    "\n",
    "**slice out the first 3 rows of arr_data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = arr_data[...]\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=goldenrod>**_Code for you to write_**</font>\n",
    "\n",
    "**slice out the last 2 columns of arr_data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = arr_data[..]\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slicing a `DataFrame` is a bit different because you can reference the index labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice out the first 10 rows of EQ_data\n",
    "EQ_catalog[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice out the a chunk of Depths\n",
    "EQ_catalog['Depth'][5:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you just want the values from that chunk and not the index labels use `.values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EQ_catalog['Depth'].values[5:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use `reindex` to rearrange or add/delete DataFrame index labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame2 = frame.reindex(['a','b','c','d'])\n",
    "frame2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Boolean indexing to filter out values from our DataFrame where the argument we want is `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EQ_catalog['Depth'] < 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=goldenrod>**_Code for you to write_**</font>\n",
    "\n",
    "**Use Boolean Indexing to filter out data so that we are only looking at rows with magnitudes larger than 6.5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames can be sorted by the values in a given column (`.sort_values`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EQ_catalog.sort_values(by=['Magnitude']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can reverse the order of sorting with `ascending=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EQ_catalog.sort_values(by=['Magnitude'],ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=goldenrod>**_Code for you to write_**</font>\n",
    "\n",
    "**sort the Earthquake data by depth. Either make ```ascending=False``` or look at the ```.tail``` to report the deepest earthquake in the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write the depth of the deepest earthquake here:**\n",
    "\n",
    "xxxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning and inspection - replacing data, removing data, find duplicates and missing data (NaNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.drop()` can be used to drop whole columns from a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EQ_data_concise = EQ_catalog.drop(['MagType','NbStations','Gap', 'Distance','RMS','Source','EventID'], axis='columns')\n",
    "EQ_data_concise.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.unique()` returns the unique values from the specified object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_mags = EQ_data_concise['Magnitude'].unique()\n",
    "unique_mags.sort()\n",
    "unique_mags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.value_counts()` returns the count of each unique value from the specified object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EQ_data_concise['Magnitude'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This functionality can be used to find duplicate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EQ_data_concise['DateTime'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EQ_data_concise[EQ_data_concise['DateTime'] == '2001/03/07 02:49:42.87']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two earthquakes at the same time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding NAN\n",
    "\n",
    "`np.isnan` returns a boolean object with True where NaNs appear in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(EQ_catalog['Distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "~np.isnan(EQ_catalog['Distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EQ_catalog[~np.isnan(EQ_catalog['Distance'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part 2: Marine Geology - Bathymetry and Magnetic Anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll look at marine magnetics and bathymetry data from two surveys, one from the Mid-Atlantic Ridge and one from the East Pacific Rise.\n",
    "\n",
    "First we'll load the Atlantic data (https://www.ngdc.noaa.gov/ships/melville/VANC05MV_mb.html), and then we'll need to clean it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the seafloor depth, marine mag anom data\n",
    "# Source: https://maps.ngdc.noaa.gov/viewers/geophysics/\n",
    "#names=['SURVEY_ID','TIMEZONE','DATE','TIME','LAT','LON','POS_TYPE','NAV_QUALCO','BAT_TTIME','CORR_DEPTH','BAT_CPCO','BAT_TYPCO','BAT_QUALCO','MAG_TOT','MAG_TOT2','MAG_RES','MAG_RESSEN','MAG_DICORR','MAG_SDEPTH','MAG_QUALCO','GRA_OBS','EOTVOS','FREEAIR','GRA_QUALCO','LINEID','POINTID'])\n",
    "\n",
    "atlantic_data = pd.read_table('data_tracks/vanc05mv.m77t')\n",
    "atlantic_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll use `np.isnan` to remove rows were we don't have depth AND magnetic field measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlantic_data_clean = atlantic_data[~np.isnan(atlantic_data['CORR_DEPTH']) &  ~np.isnan(atlantic_data['MAG_TOT'])];\n",
    "atlantic_data_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our data!\n",
    "\n",
    "<font color=goldenrod>**_Code for you to write_**</font>\n",
    "\n",
    "**Plot atlantic_data on a map and make it have a linewidth of 2 and a color of orange**\n",
    "\n",
    "**Plot atlantic_data on the same map and make it have a linewidth of 4 and a color of red**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1,(15,15))\n",
    "ax = plt.axes(projection=ccrs.Robinson())\n",
    "ax.set_global()\n",
    "    \n",
    "ax.coastlines()\n",
    "ax.stock_img()\n",
    "ax.gridlines()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1,(20,4))\n",
    "plt.plot(atlantic_data_clean['LON'],-1*atlantic_data_clean['CORR_DEPTH'],color='mediumblue');\n",
    "plt.xlabel('Longitude, degrees');\n",
    "plt.ylabel('Bathymetry, m');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1,(20,4))\n",
    "plt.plot(atlantic_data_clean['LON'],atlantic_data_clean['MAG_TOT'],color='mediumblue');\n",
    "plt.xlabel('Longitude, degrees');\n",
    "plt.ylabel('Total magnetic field, nT');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=goldenrod>**_Code for you to write_**</font>\n",
    "\n",
    "Let's just analyze the portion of the survey from around the ridge, so from longitudes -24.0 to 0.0 degrees. So we'll use Boolean indexing to pull out rows of `atlantic_data_clean` where `atlantic_data_clean['LON']` is between those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlantic_data_cropped = atlantic_data_clean[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlantic_lat = atlantic_data_cropped['LAT']\n",
    "atlantic_lon = atlantic_data_cropped['LON']\n",
    "atlantic_depth = atlantic_data_cropped['CORR_DEPTH']\n",
    "atlantic_total_mag = atlantic_data_cropped['MAG_TOT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a map of where our survey line was collected with a grid of seafloor bathymetry in the background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/MAR_track_map.png\" width=900>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1,(20,4))\n",
    "plt.plot(atlantic_lon,-1*atlantic_depth,color='mediumblue')\n",
    "plt.title('Mid Atlantic Ridge')\n",
    "plt.xlabel('Longitude, degrees')\n",
    "plt.ylabel('Bathymetry, m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1,(20,4))\n",
    "plt.plot(atlantic_lon,atlantic_total_mag,color='mediumblue')\n",
    "plt.title('Mid Atlantic Ridge')\n",
    "plt.xlabel('Longitude, degrees')\n",
    "plt.ylabel('Total magnetic field, nT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maggie Avery (a Berkeley postdoc who is an expert in marine magnetic data) used another program to project the latitude and longitude coordinates to distance from the ridge along the ship track azimuth -- let's load that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_atlantic_data = pd.read_csv('data_tracks/projected_vanc05mv.csv',names=['DIST','DEPTH','MAG_TOT'])\n",
    "atl_ridge_dist = projected_atlantic_data['DIST']\n",
    "atl_ridge_depth = projected_atlantic_data['DEPTH']\n",
    "atl_ridge_total_mag=projected_atlantic_data['MAG_TOT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1,(20,4))\n",
    "plt.plot(atl_ridge_dist,-1*atl_ridge_depth,color='mediumblue');\n",
    "plt.title('Mid Atlantic Ridge')\n",
    "plt.xlabel('Distance to Ridge, km');\n",
    "plt.ylabel('Bathymentry, m');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1,(20,4))\n",
    "plt.plot(atl_ridge_dist,atl_ridge_total_mag,color='mediumblue');\n",
    "plt.title('Mid Atlantic Ridge')\n",
    "plt.xlabel('Distance to Ridge, km');\n",
    "plt.ylabel('Total magnetic field, nT');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load and clean the data from the East Pacific Rise (https://catalog.data.gov/dataset/multibeam-collection-for-nbp9707-multibeam-data-collected-aboard-nathaniel-b-palmer-from-1997-1). This time we'll select date from Longitudes between -126.0 and -95.0 degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the seafloor depth, marine mag anom data\n",
    "# Source: https://maps.ngdc.noaa.gov/viewers/geophysics/\n",
    "#names=['SURVEY_ID','TIMEZONE','DATE','TIME','LAT','LON','POS_TYPE','NAV_QUALCO','BAT_TTIME','CORR_DEPTH','BAT_CPCO','BAT_TYPCO','BAT_QUALCO','MAG_TOT','MAG_TOT2','MAG_RES','MAG_RESSEN','MAG_DICORR','MAG_SDEPTH','MAG_QUALCO','GRA_OBS','EOTVOS','FREEAIR','GRA_QUALCO','LINEID','POINTID'])\n",
    "\n",
    "pacific_data_file=pd.read_table('data_tracks/nbp9707.m77t')\n",
    "\n",
    "pacific_data_clean = pacific_data_file[...] #use ~np.isnan to clear out rows were there are nans\n",
    "pacific_data_cropped = pacific_data_clean[...] # use Boolean indexing to select rows with Longitude -126 deg to -95 deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pacific_data_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pac_ridge_lat = pacific_data_cropped['LAT']\n",
    "pac_ridge_lon = pacific_data_cropped['LON']\n",
    "pac_ridge_depth = pacific_data_cropped['CORR_DEPTH']\n",
    "pac_ridge_total_mag = pacific_data_cropped['MAG_TOT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a map of where our survey line was collected with a grid of seafloor bathymetry in the background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/EPR_track_map.png\" width=900>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1,(20,4))\n",
    "plt.plot(pac_ridge_lon,-1*pac_ridge_depth,color='tomato');\n",
    "plt.title('East Pacific Rise');\n",
    "plt.xlabel('Longitude, degrees');\n",
    "plt.ylabel('Bathymetry, m');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1,(20,4))\n",
    "plt.plot(pac_ridge_lon,pac_ridge_total_mag,color='tomato');\n",
    "plt.title('East Pacific Rise');\n",
    "plt.xlabel('Longitude, degrees');\n",
    "plt.ylabel('Total magnetic field, nT');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, Maggie Avery used another program to project the latitude and longitude coordinates to distance from the ridge along the ship track azimuth -- let's load that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_pacific_data = pd.read_csv('data_tracks/projected_nbp9707.csv',names=['DIST','DEPTH','MAG_TOT'])\n",
    "pac_ridge_dist = projected_pacific_data['DIST']\n",
    "pac_ridge_depth = projected_pacific_data['DEPTH']\n",
    "pac_ridge_total_mag = projected_pacific_data['MAG_TOT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bathymetry__\n",
    "\n",
    "Now let's compare the two ridges' bathymetry. \n",
    "\n",
    "Let's plot them together on one figure as subplots. First we use `.GridSpec` to set up the grid of subplots, then we use `fig.add_subplot` to set up the subplot axes, and then we can start adding our plot elements to the subplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1 = plt.subplot(2,1,1)\n",
    "\n",
    "ax1.plot(pac_ridge_dist,-pac_ridge_depth,color='tomato') # plot the pacific bathymetry\n",
    "ax1.set_xlim(-1000, 1000) # set the x axis range\n",
    "ax1.set_ylim(-5000, -1000) # set the y  axis range\n",
    "ax1.set_xlabel('Distance to Ridge, km') # labels!\n",
    "ax1.set_ylabel('Bathymetry, m')\n",
    "ax1.set_title('East Pacific Rise')\n",
    "ax1.grid()\n",
    "\n",
    "ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "ax2.plot(atl_ridge_dist,-atl_ridge_depth,color='mediumblue'); # plot the atlantic bathymetry\n",
    "ax2.set_xlim(-1000, 1000)\n",
    "ax2.set_ylim(-5000, -1000)\n",
    "ax2.set_xlabel('Distance to Ridge, km')\n",
    "ax2.set_ylabel('Bathymetry, m')\n",
    "ax2.set_title('Mid Atlantic Ridge')\n",
    "ax2.grid()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/spreading_ridges.png\" width=900>\n",
    "> Source: Essentials of Geology (13th Edition) Lutgens, Tarbuck, and Tasa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe in the bathymetry? Do these ridges have a rift valley at the center? Is the slope steep or gentle? Is the bathymetry rough or smooth?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write your answer here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the ridge bathymetry, which spreading center do you think is spreading faster the Atlantic (blue) or Pacific (red)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discuss with the person next to you** and *Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Crustal Magnetic Field__\n",
    "\n",
    "Now let's compare their marine magnetic field data.\n",
    "\n",
    "<font color=goldenrod>**_Code for you to write_**</font>\n",
    "\n",
    "Make a plot of the magnetic field from each ridge (MAG_TOT). Follow the code above for the bathymetry. Be sure to include axis labels.\n",
    "\n",
    "```\n",
    "ax2.set_xlabel('Distance to Ridge, km');\n",
    "ax2.set_ylabel('Total Field, nT');\n",
    "ax2.set_title('Mid Atlantic Ridge');\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the magnetic field (follow the depth example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm defining a new function `total2anom` to process these total magnetic field measurements into magnetic anomaly by removing the background drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_to_anomaly(total_mag, distance):\n",
    "    \"\"\"\n",
    "    Simple function (i.e. too simple as it doesn't use knowledge of background field from observatory) to process \n",
    "    measured total magnetic field to magnetic anomaly. Detrends and highpass filters the total field.\n",
    "    \n",
    "    parameters\n",
    "    ----------\n",
    "    total_mag : total magnetic field measurement\n",
    "    distance : distance from the ridge in km\n",
    "    \n",
    "    output\n",
    "    ------\n",
    "    filtered_anom : marine magnetic anomaly (detrended and filtered total field)\n",
    "    \"\"\"\n",
    "    total_detrended = signal.detrend(total_mag); # detrend to remove drift\n",
    "    sample_dist = np.mean(abs(distance.values[1:]-distance.values[0:-1])); # determine sample spacing\n",
    "    fs = 1/sample_dist; # sampling frequency in km^-1\n",
    "    fN = fs *0.5; # Nyquist frequency\n",
    "    # design filter coefficents for highpass filter - 0 to 1/500km filtered, 1/450km to fN passed, \n",
    "    # remove nonlinear drift\n",
    "    filter_coefs = signal.remez(1001, [0, 0.002, 0.00222, fN], [0, 1], fs=fs);\n",
    "    # apply the filter to the detrended anomaly\n",
    "    filtered_anom = signal.filtfilt(filter_coefs, [1], total_detrended, padlen=len(total_detrended)-1)\n",
    "    \n",
    "    return filtered_anom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this `total_to_anomaly` function to compute the marine magnetic anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atl_mma = total_to_anomaly(atl_ridge_total_mag,atl_ridge_dist)\n",
    "pac_mma = total_to_anomaly(pac_ridge_total_mag,atl_ridge_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot up these data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1 = plt.subplot(2,1,1)\n",
    "\n",
    "ax1.plot(pac_ridge_dist[:],np.zeros(pac_ridge_dist.shape),color='black'); # plot a black reference line at 0 nT\n",
    "ax1.plot(pac_ridge_dist,pac_mma,color='tomato'); # plot the pacific marine magnetic anomaly\n",
    "ax1.set_xlim(-1000, 1000);\n",
    "ax1.set_xlabel('Distance to Ridge, km');\n",
    "ax1.set_ylabel('Magnetic Anomaly, nT');\n",
    "ax1.set_title('East Pacific Rise');\n",
    "\n",
    "ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "ax2.plot(atl_ridge_dist[:],np.zeros(atl_ridge_dist.shape),color='black'); # plot a black reference line at 0 nT\n",
    "ax2.plot(atl_ridge_dist,atl_mma,color='mediumblue'); # plot the atlantic marine magnetic anomaly\n",
    "ax2.set_xlim(-1000, 1000);\n",
    "ax2.set_xlabel('Distance to Ridge, km');\n",
    "ax2.set_ylabel('Magnetic Anomaly, nT');\n",
    "ax2.set_title('Mid Atlantic Ridge');\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=goldenrod>**_Code for you to write_**</font>\n",
    "\n",
    "Plot the marine magnetic anomalies together as subplots again with reference lines at zero nT, but zoom in the `xlim` to $\\pm$250 km for the pacific data and $\\pm$150 km for the atlantic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which wiggles can you match between lines and to the model profile due to the GPTS below? Can you pick the Bruhnes, Matuyama, Gauss, and Gilbet polarity chrons? What distance from the ridge does the Bruhnes-Matuyama reversal (which tells us an age of 780 kyr) occur at for both ridges?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write your answer here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/marine_mag_anom.png\" width=900>\n",
    "> Source: Fundamentals of Geophysics (2nd Edition) Lowrie, W."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the marine magnetic anomalies, which spreading center do you think is spreading faster the Atlantic (blue) or Pacific (red)? Is that consistent with your estimate from the bathymetry?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write your answer here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Build a new DataFrame of our Distance, Depth, Marine Magnetic Anomaly output as a .csv file that we can open again later.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atl_mma_data = pd.DataFrame({'Distance':atl_ridge_dist, 'Depth':atl_ridge_depth, 'Magnetic_Anomaly':atl_mma})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atl_mma_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atl_mma_data.to_csv(\"Atlantic_dist_depth_mma.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pac_mma_data = pd.DataFrame({'Distance':pac_ridge_dist, 'Depth':pac_ridge_depth, 'Magnetic_Anomaly':pac_mma})\n",
    "pac_mma_data.to_csv(\"Pacific_dist_depth_mma.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload the Pacific_dist_depth_mma.csv to bCourses\n",
    "\n",
    "https://bcourses.berkeley.edu/courses/1482009/assignments/8026222?module_item_id=15771585"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
