{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to machine learning: classification of basalt source\n",
    "\n",
    "## Import scientific python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning\n",
    "Text from: https://scikit-learn.org/stable/tutorial/basic/tutorial.html\n",
    "\n",
    "In general, a learning problem considers a set of n samples of data and then tries to predict properties of unknown data. If each sample is more than a single number and, for instance, a multi-dimensional entry (aka multivariate data), it is said to have several attributes or features.\n",
    "\n",
    "Learning problems fall into a few categories:\n",
    "- **supervised learning**, in which the data comes with additional attributes that we want to predict (Click here to go to the scikit-learn supervised learning page).This problem can be either:\n",
    "    - *classification*: samples belong to two or more classes and we want to learn from already labeled data how to predict the class of unlabeled data. An example of a classification problem would be handwritten digit recognition, in which the aim is to assign each input vector to one of a finite number of discrete categories. Another way to think of classification is as a discrete (as opposed to continuous) form of supervised learning where one has a limited number of categories and for each of the n samples provided, one is to try to label them with the correct category or class.\n",
    "    - *regression*: if the desired output consists of one or more continuous variables, then the task is called regression. An example of a regression problem would be the prediction of the length of a salmon as a function of its age and weight.\n",
    "\n",
    "- **unsupervised learning**, in which the training data consists of a set of input vectors x without any corresponding target values. The goal in such problems may be to discover groups of similar examples within the data, where it is called clustering, or to determine the distribution of data within the input space, known as density estimation, or to project the data from a high-dimensional space down to two or three dimensions for the purpose of visualization (Click here to go to the Scikit-Learn unsupervised learning page).\n",
    "\n",
    "### Training set and testing set\n",
    "\n",
    "Machine learning is about learning some properties of a data set and then testing those properties against another data set. A common practice in machine learning is to evaluate an algorithm by splitting a data set into two. We call one of those sets the training set, on which we learn some properties; we call the other set the testing set, on which we test the learned properties.\n",
    "\n",
    "**Today we will focus on classification through a supervised learning approach**\n",
    "\n",
    "*Systems doing this type of analysis are all around us. Consider a spam filter for example*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying volcanic rocks\n",
    "\n",
    "<img src=\"./images/volcanic-tectonics.png\" width = 600 align = 'center'>\n",
    "\n",
    "Today we are going to continue dealing with igneous geochemistry data. Igneous rocks are those that crystallize from cooling magma. Different magmas have different compositions associated with their origin as we explore two weeks ago. During class today, we will continue to focus on data from mafic lava flows (these are called basalts and are the relatively low silica, high iron end of what we looked at two weeks ago).\n",
    "\n",
    "> Igneous rocks form in a wide variety of tectonic settings,\n",
    "including mid-ocean ridges, ocean islands, and volcanic\n",
    "arcs. It is a problem of great interest to igneous petrologists\n",
    "to recover the original tectonic setting of mafic rocks of the\n",
    "past. When the geological setting alone cannot unambiguously\n",
    "resolve this question, the chemical composition of\n",
    "these rocks might contain the answer. The major, minor,\n",
    "and trace elemental composition of basalts shows large\n",
    "variations, for example as a function of formation depth\n",
    "(e.g., Kushiro and Kuno, 1963) --- *Vermeesch (2006)*\n",
    "\n",
    "For this analysis we are going to use a dataset that was compiled in \n",
    "\n",
    "Vermeesch (2006) Tectonic discrimination of basalts with classification trees, *Geochimica et Cosmochimica Acta*  https://doi.org/10.1016/j.gca.2005.12.016\n",
    "\n",
    "These data were grouped into 3 categories:\n",
    "\n",
    "- 256 ***Island arc basalts (IAB)*** from the Aeolian, Izu-Bonin, Kermadec, Kurile, Lesser Antilles, Mariana, Scotia, and Tonga arcs.\n",
    "- 241 ***Mid-ocean ridge (MORB)*** samples from the East Pacific Rise, Mid Atlantic Ridge, Indian Ocean, and Juan de Fuca Ridge.\n",
    "- 259 ***Ocean-island (OIB)*** samples from St. Helena, the Canary, Cape Verde, Caroline, Crozet, Hawaii-Emperor, Juan Fernandez, Marquesas, Mascarene, Samoan, and Society islands.\n",
    "\n",
    "**Let's look at the illustration above and determine where each of these settings are within a plate tectonic context**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "\n",
    "\n",
    "The data are from the supplemental materials of the Vermeesch (2006) paper. The samples are grouped by affinity MORB, OIB, and IAB. They are additionally assigned affinity codes and colors from the default matplotlib cycle:\n",
    "\n",
    "|affinity| affinity code | color |\n",
    "|--------|---------------|-------|\n",
    "| MORB| 0 | C0\n",
    "| OIB |  1 | C1\n",
    "| IAB |  2 | C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basalt_data = pd.read_csv('./data/Vermeesch2006.csv')\n",
    "basalt_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MORB_data = basalt_data[basalt_data['affinity']=='MORB']\n",
    "OIB_data = basalt_data[basalt_data['affinity']=='OIB']\n",
    "IAB_data = basalt_data[basalt_data['affinity']=='IAB']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can geochemical data be used to classify the tectonic setting?\n",
    "\n",
    "These data are labeled. The author already determined what setting these basalts came from. However, is there are way that we could use these labeled data to determine the setting for an unknown basalt?\n",
    "\n",
    "A paper published in 1982 proposed that the elements titanium and vanadium were particular good at giving insight into tectonic setting. The details of why are quite complicated and can be summarized as \"the depletion of V relative to Ti is a function of the fO2 of the magma and its source, the degree of partial melting, and subsequent fractional crystallization.\" If you take EPS100B you will learn more about the fundamentals behind this igneous petrology. *For the moment you can consider the working hypothesis behind this classification to that different magmatic environments have differences in oxidation states that are reflected in Ti vs V ratios.*\n",
    "\n",
    "Shervais, J.W. (1982) Ti-V plots and the petrogenesis of modern and ophiolitic lavas *Earth and Planetary Science Letters* https://doi.org/10.1016/0012-821X(82)90120-0\n",
    "\n",
    "### Plot TiO2 (wt%) vs V (ppm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(MORB_data['TiO2 (wt%)'],MORB_data['V (ppm)'],label='mid-ocean ridge',edgecolors='black')\n",
    "plt.scatter(OIB_data['TiO2 (wt%)'],OIB_data['V (ppm)'],label='ocean island',edgecolors='black')\n",
    "plt.scatter(IAB_data['TiO2 (wt%)'],IAB_data['V (ppm)'],label='island arc',edgecolors='black')\n",
    "plt.xlabel('TiO2 (wt%)')\n",
    "plt.ylabel('V (ppm)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "scatter_plot = plt.scatter(basalt_data['TiO2 (wt%)'],basalt_data['V (ppm)'],color=basalt_data['color'],edgecolors='black')\n",
    "plt.xlabel('TiO2 (wt%)')\n",
    "plt.ylabel('V (ppm)')\n",
    "plt.title('MORB (blue), OIB (orange), IAB (green)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the pandas groupby function to group by affinity and describe the values of one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "basalt_data.groupby('affinity')['TiO2 (wt%)'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CODE FOR YOU TO WRITE: Use the groupby command and describe the grouped vanadium concentration for the data.**\n",
    "\n",
    "*Can we different between the different affinities on vanadium concentration alone?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eye test classification method\n",
    "\n",
    "In order to classify the basalt into their affinity based on titanium and vanadium concentrations, we can use a classification method.\n",
    "\n",
    "The goal here is to be able to make an inference of what environment an unknown basalt formed in based on comparison to these data.\n",
    "\n",
    "Let's say that we have two points where there affinity is unknown.\n",
    "- point 1 has TiO2 of 4% and V concentration of 300 ppm\n",
    "- point 2 has TiO2 of 1% and V concentration of 350 ppm\n",
    "- point 3 has TiO2 of 1.9% and V concentration of 200 ppm\n",
    "\n",
    "**Let's take votes on how they should be classified**\n",
    "\n",
    "***WRITE HOW YOU THINK THEY SHOULD BE CLASSIFIED HERE***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_1_TiO2 = 4\n",
    "point_1_V = 300\n",
    "point_2_TiO2 = 1\n",
    "point_2_V = 350\n",
    "point_3_TiO2 = 1.9\n",
    "point_3_V = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(MORB_data['TiO2 (wt%)'],MORB_data['V (ppm)'],label='mid-ocean ridge',edgecolors='black')\n",
    "plt.scatter(OIB_data['TiO2 (wt%)'],OIB_data['V (ppm)'],label='ocean island',edgecolors='black')\n",
    "plt.scatter(IAB_data['TiO2 (wt%)'],IAB_data['V (ppm)'],label='island arc',edgecolors='black')\n",
    "plt.scatter(point_1_TiO2,point_1_V,label='unknown point 1',color='black',marker='d',s=100)\n",
    "plt.scatter(point_2_TiO2,point_2_V,label='unknown point 2',color='red',marker='>',s=100)\n",
    "plt.scatter(point_3_TiO2,point_3_V,label='unknown point 2',color='yellow',edgecolors='black',marker='s',s=100)\n",
    "plt.xlabel('TiO2 (wt%)')\n",
    "plt.ylabel('V (ppm)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbors Classification\n",
    "\n",
    "In nearest neighbors classification, classification is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point. There are different ways this can be done and can be weighted.\n",
    "\n",
    "### Filter the data to ones that have Ti and V data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basalt_data_Ti_V = basalt_data[(~basalt_data['TiO2 (wt%)'].isna()) & (~basalt_data['V (ppm)'].isna())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the data\n",
    "\n",
    "Given that the nearest neighbor is a distance and that they y-axis and x-axis are so different (in part because of different units) we need to normalize the data. We will divide the 'TiO2 (wt%)' by the maximum 'TiO2 (wt%)' to get a value between 0 and 1. We will do the same for V (ppm) as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basalt_data_Ti_V.loc[:,'Ti_norm'] = basalt_data['TiO2 (wt%)']/np.max(basalt_data['TiO2 (wt%)'])\n",
    "basalt_data_Ti_V.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code for you to write**\n",
    "\n",
    "Make a column called V_norm that is normalized vanadium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSTRUCTOR VERSION\n",
    "basalt_data_Ti_V.loc[:,'V_norm'] = basalt_data['V (ppm)']/np.max(basalt_data['V (ppm)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code for you to write**\n",
    "\n",
    "Make a scatter plot of Ti_norm vs V_norm that is colored by affinity (`color=basalt_data_Ti_V['color']`). It should look a lot like the previous scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSTRUCTOR VERSION\n",
    "scatter_plot = plt.scatter(basalt_data_Ti_V['Ti_norm'],basalt_data_Ti_V['V_norm'],color=basalt_data_Ti_V['color'],edgecolors='black')\n",
    "plt.xlabel('Ti_norm')\n",
    "plt.ylabel('V_norm')\n",
    "plt.title('MORB (blue), OIB (orange), IAB (green)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing arrays of the data\n",
    "\n",
    "We will make a 2 x n array of the TiO2 (wt%) and V (ppm) values (where n is the number of data points) and a 1 x n array of the classifications (the tectonic affinities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basalt_Ti_V = basalt_data_Ti_V[['Ti_norm', 'V_norm']].values\n",
    "basalt_Ti_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basalt_affinity = basalt_data_Ti_V['affinity'].tolist()\n",
    "basalt_affinity_code = basalt_data_Ti_V['affinity code'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import sci-kit learn \n",
    "\n",
    "We will be using the scikit-learn library which is a widely used library for machine learning in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our classifier\n",
    "\n",
    "We will construct a classifier that uses the 5 nearest neighbors (`n_neighbors=5`) and weight points by the inverse of their distance (`weights='distance'`) such that closer neighbors of a query point will have a greater influence than neighbors which are further away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_neighbors = neighbors.KNeighborsClassifier(n_neighbors=10, weights='distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit/train the classifier\n",
    "\n",
    "We can then feed the array of the data and the array of the classification in a `.fit` function preformed on the classifier object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_neighbors.fit(basalt_Ti_V, basalt_affinity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize our mystery points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_1_TiO2_norm = 4/np.max(basalt_data['TiO2 (wt%)'])\n",
    "point_1_V_norm = 300/np.max(basalt_data['V (ppm)'])\n",
    "point_2_TiO2_norm = 1/np.max(basalt_data['TiO2 (wt%)'])\n",
    "point_2_V_norm = 350/np.max(basalt_data['V (ppm)'])\n",
    "point_3_TiO2_norm = 1.9/np.max(basalt_data['TiO2 (wt%)'])\n",
    "point_3_V_norm = 200/np.max(basalt_data['V (ppm)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "scatter_plot = plt.scatter(basalt_data_Ti_V['Ti_norm'],basalt_data_Ti_V['V_norm'],color=basalt_data_Ti_V['color'],edgecolors='black')\n",
    "plt.scatter(point_1_TiO2_norm,point_1_V_norm,label='unknown point 1',color='black',marker='d',s=100)\n",
    "plt.scatter(point_2_TiO2_norm,point_2_V_norm,label='unknown point 2',color='red',marker='>',s=100)\n",
    "plt.scatter(point_3_TiO2_norm,point_3_V_norm,label='unknown point 2',color='yellow',edgecolors='black',marker='s',s=100)\n",
    "plt.xlabel('Ti_norm')\n",
    "plt.ylabel('V_norm')\n",
    "plt.gca().set_aspect('equal', 'box')\n",
    "plt.title('MORB (blue), OIB (orange), IAB (green)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the tectonic affinity of the mystery points using the neighbors classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_neighbors.predict([[point_1_TiO2_norm,point_1_V_norm],\n",
    "                             [point_2_TiO2_norm,point_2_V_norm],\n",
    "                             [point_3_TiO2_norm,point_3_V_norm]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit/train using the basalt_affinity_code rather than the string names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_neighbors.fit(basalt_Ti_V, basalt_affinity_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_neighbors.predict([[point_1_TiO2_norm,point_1_V_norm],\n",
    "                    [point_2_TiO2_norm,point_2_V_norm],\n",
    "                    [point_3_TiO2_norm,point_3_V_norm]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the decision boundary\n",
    "\n",
    "Let's make a 101 x 101 grid of x and y values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(np.linspace(0, 1, 101),\n",
    "                     np.linspace(0, 1, 101))\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(xx, yy, s=1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_classes = classifier_neighbors.predict(grid)\n",
    "grid_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = ListedColormap(['C0', 'C1', 'C2'])\n",
    "grid_classes = grid_classes.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "\n",
    "plt.pcolormesh(xx, yy, grid_classes, cmap=cmap)\n",
    "scatter_plot = plt.scatter(basalt_data_Ti_V['Ti_norm'],basalt_data_Ti_V['V_norm'],\n",
    "                           color=basalt_data_Ti_V['color'],edgecolors='black')\n",
    "\n",
    "plt.scatter(point_1_TiO2_norm,point_1_V_norm,label='unknown point 1',color='black',marker='d',s=100)\n",
    "plt.scatter(point_2_TiO2_norm,point_2_V_norm,label='unknown point 2',color='red',marker='>',s=100)\n",
    "plt.scatter(point_3_TiO2_norm,point_3_V_norm,label='unknown point 2',color='yellow',edgecolors='black',marker='s',s=100)\n",
    "\n",
    "plt.xlabel('Ti_norm')\n",
    "plt.ylabel('V_norm')\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.gca().set_aspect('equal', 'box')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing\n",
    "\n",
    "How good is our nearest neighbor classifier? To answer this we'll need to find out how frequently our classifications are correct.\n",
    "\n",
    "**Discussion question**\n",
    "\n",
    "*How should be determine the accuracy of this classification scheme using the data that we already have?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(basalt_data_Ti_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a training and testing data set\n",
    "\n",
    "There are 514 rows with data. Let's use a random half of them for training and the other half for testing. To do this, we'll shuffle all the rows, take the first 257 as the training set, and the remaining 257 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a randomly ordered dataframe from the initial one\n",
    "randomized_basalt_data = basalt_data_Ti_V.sample(frac=1) \n",
    "\n",
    "# Take the first 257 data points to use for \"training\"\n",
    "training_data = copy.deepcopy(randomized_basalt_data.iloc[0:257])\n",
    "\n",
    "# Use the rest to apply our machine learning on\n",
    "remaining_data = copy.deepcopy(randomized_basalt_data.iloc[257:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basalt_Ti_V_training = training_data[['Ti_norm', 'V_norm']].values\n",
    "basalt_Ti_V_remaining = remaining_data[['Ti_norm', 'V_norm']].values\n",
    "basalt_affinity_training = training_data['affinity code'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_neighbors.fit(basalt_Ti_V_training, basalt_affinity_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the classification regions fit with half the data\n",
    "\n",
    "We can send the grid to the classifier to see the classification regions and decision boundary that has been fit with half of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_classes = classifier_neighbors.predict(grid)\n",
    "grid_classes = grid_classes.reshape(xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.pcolormesh(xx, yy, grid_classes, cmap=cmap)\n",
    "plt.xlabel('Ti_norm')\n",
    "plt.ylabel('V_norm')\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.gca().set_aspect('equal', 'box')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the remaining data (test data) to the classification regions\n",
    "\n",
    "Place the test data on this graph and you can see at once that while the classifier got many of the points right, there are some mis-classified points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.pcolormesh(xx, yy, grid_classes, cmap=cmap)\n",
    "\n",
    "plt.scatter(remaining_data['Ti_norm'],remaining_data['V_norm'],\n",
    "                           color=remaining_data['color'],edgecolors='black')\n",
    "\n",
    "plt.xlabel('Ti_norm')\n",
    "plt.ylabel('V_norm')\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.gca().set_aspect('equal', 'box')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the accuracy of the classifier\n",
    "\n",
    "Since the test set was chosen randomly from the original sample it should preform with similar accuracy on the overall population. Let's calculate the success rate of the classification.\n",
    "\n",
    "We will input the remaining data (test data) to the classifier and then assign these classified affinities to a new column in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_classes = classifier_neighbors.predict(basalt_Ti_V_remaining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_data['predicted_class'] = remaining_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a new column of the classified affinities for the test data. We also have the actually affinities given that the data were originally labeled with classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_data['correct_assignment'] = remaining_data['predicted_class'].eq(remaining_data['affinity code'])\n",
    "remaining_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_data['correct_assignment'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using scikit-learn functions to get an accuracy score of this nearest neighbor approach\n",
    "\n",
    "Given that this approach of randomly splitting the data into training and test groups is quite common in macine learning classification, there are built-in convenience functions that can be used to more compactly do the same operations that we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# split the data with 50% in each set\n",
    "X1, X2, y1, y2 = train_test_split(basalt_Ti_V, basalt_affinity_code,train_size=0.5)\n",
    "\n",
    "# fit the model on one set of data\n",
    "classifier_neighbors.fit(X1, y1)\n",
    "\n",
    "# evaluate the model on the second set of data\n",
    "y2_model = classifier_neighbors.predict(X2)\n",
    "accuracy_score(y2, y2_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other classification algorithms\n",
    "\n",
    "If you go to the scikit-learn homepage you will find many available classifiers: https://scikit-learn.org/stable/index.html. They are nicely illustrated in this code from the scikit-learn documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code source: Gaël Varoquaux\n",
    "#              Andreas Müller\n",
    "# Modified for documentation by Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1)\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "datasets = [make_moons(noise=0.3, random_state=0),\n",
    "            make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "            linearly_separable\n",
    "            ]\n",
    "\n",
    "figure = plt.figure(figsize=(27, 9))\n",
    "i = 1\n",
    "# iterate over datasets\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # preprocess dataset, split into training and test part\n",
    "    X, y = ds\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=.4, random_state=42)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "    if ds_cnt == 0:\n",
    "        ax.set_title(\"Input data\")\n",
    "    # Plot the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "               edgecolors='k')\n",
    "    # Plot the testing points\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n",
    "               edgecolors='k')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "\n",
    "        # Plot the decision boundary. For that, we will assign a color to each\n",
    "        # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "        if hasattr(clf, \"decision_function\"):\n",
    "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        else:\n",
    "            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "        # Plot the training points\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "                   edgecolors='k')\n",
    "        # Plot the testing points\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "                   edgecolors='k', alpha=0.6)\n",
    "\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        if ds_cnt == 0:\n",
    "            ax.set_title(name)\n",
    "        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
    "                size=15, horizontalalignment='right')\n",
    "        i += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a word of warning, we shouldn't get too carried away. Clearly, there are complexities related to this approach (our accuracy scores aren't that high). Shervais notes that: \n",
    "> \"More specific evaluation of the tectonic setting of these and other ophiolites requires\n",
    "application of detailed geologic and petrologic data as well as geochemistry. The Ti/V discrimination diagram, however,\n",
    "is a potentially powerful adjunct to these techniques.\"\n",
    "\n",
    "Additionally, we would like to be able to assign physical processes to the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot = plt.scatter(basalt_data['TiO2 (wt%)'],basalt_data['V (ppm)'],color=basalt_data['color'],edgecolors='black')\n",
    "plt.xlabel('TiO2 (wt%)')\n",
    "plt.ylabel('V (ppm)')\n",
    "plt.title('MORB (blue), OIB (orange), IAB (green)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a linear classifier\n",
    "\n",
    "Rather than using a nearest neighbor approach we could instead implement hard cut offs as lines using a linear classifier. A benefit of using such a classifier is that the data do not need to be normalized between 0 and 1. Instead, the actual values can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier_svc = SVC(kernel=\"poly\",degree=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basalt_Ti_V_unnorm = basalt_data_Ti_V[['TiO2 (wt%)','V (ppm)']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code for you to write: implement the classifier_svc and determine its accuracy using a training set and a test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code for you to write: pass a grid to the classifier and plot with the data**\n",
    "\n",
    "We will want to use a grid that is spaced according to the non-normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(np.linspace(0, 5, 101),\n",
    "                     np.linspace(0, 600, 101))\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
